{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from training_electromagnetic import Trainer\n",
    "from presets import LambdaSearch\n",
    "from config_plots import global_settings\n",
    "from utils import *\n",
    "from parameters import Params\n",
    "\n",
    "global_settings()\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_logs = len(\n",
    "    [\n",
    "        name for name in os.listdir(\n",
    "            '../logs/'\n",
    "        ) if os.path.isfile(name)\n",
    "    ]\n",
    ")\n",
    "\n",
    "if num_logs > 2000:\n",
    "    free_logs('../logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Parameters and Start Tasks\n",
    "\n",
    "This part includes customized training parameters. Change these to whatever you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_presets = LambdaSearch(data_path='//raid/elhamod/Eignedecomposition/Electromagnetic/electromagnetic_dualphase')\n",
    "\n",
    "train_sizes = ['2000examples_400x400complex.mat']\n",
    "\n",
    "datasets = []\n",
    "break_loop = True\n",
    "loss_plot = True\n",
    "\n",
    "num_of_experiments = 1\n",
    "\n",
    "mb = master_bar(range(num_of_experiments))\n",
    "for i in mb:\n",
    "    for train_size in progress_bar(train_sizes):\n",
    "        \n",
    "        param = param_presets.NSE_DNNex_overlap()\n",
    "        param.data_params['train_size']=train_size\n",
    "        param.name = 'CoPhy_PGNN'\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.train_params['num_batch'] = 10\n",
    "        param.train_params['steplr'] = 80\n",
    "        param.train_params['gamma'] = 0.99\n",
    "        param.loss_params['lambda_e0'] = 1# Initial value of S_coeff\n",
    "        param.loss_params['lambda_s'] = 0.0# Initial value of C_coeff\n",
    "        param.loss_params['anneal_factor'] = 0.95# A lower value will decay S_coeff faster.\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.0001,# Final value of C_coeff\n",
    "            'threshold': 2000, #350\n",
    "            'smooth': 0.01,\n",
    "        }   \n",
    "        param.train_params['epochs'] = 4000\n",
    "        param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "        param.nn_params['hidden_size'] = 100\n",
    "        param.nn_params['depth'] = 2\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "# the black-box neural networks\n",
    "param = param_presets.DNN()\n",
    "param.data_params['train_size']=['2000examples_400x400complex.mat']\n",
    "param.name = 'BB'\n",
    "param.train_params['break_loop_early'] = break_loop\n",
    "param.train_params['num_batch'] = 10\n",
    "param.data_params['device'] = device\n",
    "param.nn_params['device'] = device\n",
    "param.train_params['epochs'] = 4000\n",
    "param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "param.nn_params['hidden_size'] = 100\n",
    "param.nn_params['depth'] = 2\n",
    "trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "trainer.start(param)\n",
    "\n",
    "# CoPhy-PGNN\n",
    "param = param_presets.NSE_DNNex()\n",
    "param.data_params['train_size']=['2000examples_400x400complex.mat']\n",
    "param.name = 'CoPhy'\n",
    "param.train_params['break_loop_early'] = break_loop\n",
    "param.train_params['num_batch'] = 10\n",
    "param.loss_params['lambda_e0'] = 0.01# Initial value of S_coeff\n",
    "param.loss_params['anneal_factor'] = 0.9# A lower value will decay S_coeff faster.\n",
    "param.data_params['device'] = device\n",
    "param.nn_params['device'] = device\n",
    "param.loss_params['cold_start'] = {\n",
    "    'mode': 'sigmoid',\n",
    "    'lambda_s': 0.00000006,# Final value of C_coeff\n",
    "    'threshold': 1500,\n",
    "    'smooth': 0.003\n",
    "}   \n",
    "param.train_params['epochs'] = 4000\n",
    "param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "param.nn_params['hidden_size'] = 100\n",
    "param.nn_params['depth'] = 2\n",
    "trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "trainer.start(param) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
