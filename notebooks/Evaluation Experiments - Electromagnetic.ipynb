{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Entry\n",
    "\n",
    "**Note**: This notebook generate results for evaluations. Evaluation results please go to folder 'eval_train_size'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### default nerual network parameters\n",
    "~~~~\n",
    "self.nn_params = {\n",
    "    'hidden_size': 100,\n",
    "    'depth': 3,               # network depth = 'depth' + 1\n",
    "    'activation': torch.nn.Tanh,\n",
    "    'device': device\n",
    "}\n",
    "~~~~\n",
    "### default training parameters\n",
    "~~~~\n",
    "self.train_params = {\n",
    "    'epochs': 500,\n",
    "    'train_loss': [\n",
    "        'mse_loss', \n",
    "        'phy_loss', \n",
    "        'energy_loss'\n",
    "    ],\n",
    "    'test_loss': [\n",
    "        'phy_loss', \n",
    "        'energy_loss'\n",
    "    ],\n",
    "    'num_batch': 1,\n",
    "    'optimizer': torch.optim.Adamax,\n",
    "    'cyclical': {\n",
    "        'base_lr': 0.001, \n",
    "        'max_lr': 0.1, \n",
    "        'step_size_up': 20, \n",
    "        'step_size_down': 20, \n",
    "        'mode': 'triangular'\n",
    "    },     # set to False or {} to disable\n",
    "    'L2_reg': 0.0,\n",
    "    'verbose': False,\n",
    "    'print_interval': 10,\n",
    "    'early_stopping': False\n",
    "}\n",
    "~~~~\n",
    "### default input/output parameters\n",
    "~~~~\n",
    "self.io_params = {\n",
    "    'path_out': '../results/',\n",
    "    'path_fig': '../figures/',\n",
    "    'path_log': '../logs/',\n",
    "    'env_path': [],           # path that will be registered through sys.path\n",
    "    'use_timestamp': True\n",
    "}\n",
    "~~~~\n",
    "### default dataset parameters\n",
    "~~~~\n",
    "self.data_params = {\n",
    "    'data_path': '../datasets/Electromagnetic/dataShort',\n",
    "    'normalize_input': True,\n",
    "    'normalize_output': False,\n",
    "    'device': device,\n",
    "    'dataset': 'new'\n",
    "}\n",
    "~~~~\n",
    "### default loss function parameters\n",
    "~~~~\n",
    "self.loss_params = {\n",
    "    'lambda_s': 1.0,\n",
    "    'lambda_e0': 0.2,\n",
    "    'anneal_factor': 0.9,\n",
    "    'anneal_interval': 50,    # this is a parameter also for noise and cyclical update \n",
    "    'norm_wf': False,\n",
    "    # set to False or None or {} to disable the noise\n",
    "    'noise': {'mode': 'uniform', 'mean': 0.0, 'var': 0.5, 'decay': 0.9}, \n",
    "    'cyclical': {'mode': 'sin', 'decay': 0.9, 'mean': 1.0, 'amp': 1.0, 'period': 20, 'cycle_momentum': False}\n",
    "}\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models\n",
    "\n",
    "|           Models            |      Abbr.       |\n",
    "| --------------------------- | ---------------- |\n",
    "| Deep Neural Networks        | DNN              |\n",
    "| Normalized SE-DNN$_{ex}$    | NSE-DNN$_{ex}$   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from training_electromagnetic import Trainer\n",
    "from presets import LambdaSearch\n",
    "from config_plots import global_settings\n",
    "from utils import *\n",
    "from parameters import Params\n",
    "\n",
    "global_settings()\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "torch.cuda.set_device(6)\n",
    "device = torch.device('cuda:6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_logs = len(\n",
    "    [\n",
    "        name for name in os.listdir(\n",
    "            '../logs/'\n",
    "        ) if os.path.isfile(name)\n",
    "    ]\n",
    ")\n",
    "\n",
    "if num_logs > 2000:\n",
    "    free_logs('../logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Parameters and Start Tasks\n",
    "\n",
    "This part includes customized training parameters. Change these to whatever you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_presets = LambdaSearch(data_path='//home/elhamod/CoPhy-PGNN/datasets')\n",
    "\n",
    "train_sizes = ['2000examples_400x400complex.mat'] #Other datasets ['dataShort.mat', 'data1000.mat', 'dataFull.mat']\n",
    "\n",
    "datasets = []\n",
    "break_loop = True\n",
    "loss_plot = False\n",
    "\n",
    "\n",
    "num_of_experiments = 1\n",
    "\n",
    "mb = master_bar(range(num_of_experiments))\n",
    "for i in mb:\n",
    "    for train_size in progress_bar(train_sizes):\n",
    "        \n",
    "        param = param_presets.NSE_DNNex()\n",
    "        param.data_params['train_size']=train_size\n",
    "        param.name = 'CoPhy'\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.train_params['num_batch'] = 10\n",
    "        param.loss_params['lambda_e0'] = 0.01# Initial value of S_coeff\n",
    "        param.loss_params['anneal_factor'] = 0.9# A lower value will decay S_coeff faster.\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.00000006,# Final value of C_coeff\n",
    "            'threshold': 1500,\n",
    "            'smooth': 0.003\n",
    "        }   \n",
    "        param.train_params['epochs'] = 4000\n",
    "        param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "        param.nn_params['hidden_size'] = 100\n",
    "        param.nn_params['depth'] = 2\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "param_presets = LambdaSearch(data_path='../datasets/')\n",
    "\n",
    "train_sizes = [100, 200, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
    "break_loop = False\n",
    "loss_plot = False\n",
    "mb = master_bar(range(10))\n",
    "for i in mb:\n",
    "    for train_size in progress_bar(train_sizes):\n",
    "        \n",
    "        # the black-box neural networks\n",
    "        param = param_presets.DNN()\n",
    "        param.data_params['train_size']=train_size\n",
    "        param.name = 'BB'\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.train_params['num_batch'] = 10\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.train_params['epochs'] = 4000\n",
    "        param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "        param.nn_params['hidden_size'] = 100\n",
    "        param.nn_params['depth'] = 2\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "        \n",
    "        # CoPhy-PGNN\n",
    "        param = param_presets.NSE_DNNex()\n",
    "        param.data_params['train_size']=train_size\n",
    "        param.name = 'CoPhy'\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.train_params['num_batch'] = 10\n",
    "        param.loss_params['lambda_e0'] = 0.01# Initial value of S_coeff\n",
    "        param.loss_params['anneal_factor'] = 0.9# A lower value will decay S_coeff faster.\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.00000006,# Final value of C_coeff\n",
    "            'threshold': 1500,\n",
    "            'smooth': 0.003\n",
    "        }   \n",
    "        param.train_params['epochs'] = 4000\n",
    "        param.train_params['early_stopping'] = {'patience':4000, 'verbose':False, 'delta':0}\n",
    "        param.nn_params['hidden_size'] = 100\n",
    "        param.nn_params['depth'] = 2\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
