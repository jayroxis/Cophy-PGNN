{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Entry\n",
    "\n",
    "**Note**: This notebook generate results for evaluations. Evaluation results please go to folder 'eval_train_size'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default nerual network parameters\n",
    "~~~~\n",
    "self.nn_params = {\n",
    "    'hidden_size': 100,\n",
    "    'depth': 3,               # network depth = 'depth' + 1\n",
    "    'activation': torch.nn.Tanh,\n",
    "    'device': device\n",
    "}\n",
    "~~~~\n",
    "### default training parameters\n",
    "~~~~\n",
    "self.train_params = {\n",
    "    'epochs': 500,\n",
    "    'train_loss': [\n",
    "        'mse_loss', \n",
    "        'phy_loss', \n",
    "        'energy_loss'\n",
    "    ],\n",
    "    'test_loss': [\n",
    "        'phy_loss', \n",
    "        'energy_loss'\n",
    "    ],\n",
    "    'num_batch': 1,\n",
    "    'optimizer': torch.optim.Adamax,\n",
    "    'cyclical': {\n",
    "        'base_lr': 0.001, \n",
    "        'max_lr': 0.1, \n",
    "        'step_size_up': 20, \n",
    "        'step_size_down': 20, \n",
    "        'mode': 'triangular'\n",
    "    },     # set to False or {} to disable\n",
    "    'L2_reg': 0.0,\n",
    "    'verbose': False,\n",
    "    'print_interval': 10,\n",
    "    'early_stopping': False\n",
    "}\n",
    "~~~~\n",
    "### default input/output parameters\n",
    "~~~~\n",
    "self.io_params = {\n",
    "    'path_out': '../results/',\n",
    "    'path_fig': '../figures/',\n",
    "    'path_log': '../logs/',\n",
    "    'env_path': [],           # path that will be registered through sys.path\n",
    "    'use_timestamp': True\n",
    "}\n",
    "~~~~\n",
    "### default dataset parameters\n",
    "~~~~\n",
    "self.data_params = {\n",
    "    'data_path': '../datasets/',\n",
    "    'phase': 'single-phase',  # use 'single-phase', 'dual-phase' or 'multi-phase'\n",
    "    'n_sites': 4,\n",
    "    'train_size': 20000,\n",
    "    'val_size': 2000,\n",
    "    'test_size': 0,           # set to 0 to use all the test data\n",
    "    'normalize_input': True,\n",
    "    'normalize_output': False,\n",
    "    'device': device,\n",
    "    'dataset': 'new'\n",
    "}\n",
    "~~~~\n",
    "### default loss function parameters\n",
    "~~~~\n",
    "self.loss_params = {\n",
    "    'lambda_s': 1.0,\n",
    "    'lambda_e0': 0.2,\n",
    "    'anneal_factor': 0.9,\n",
    "    'anneal_interval': 50,    # this is a parameter also for noise and cyclical update \n",
    "    'norm_wf': False,\n",
    "    # set to False or None or {} to disable the noise\n",
    "    'noise': {'mode': 'uniform', 'mean': 0.0, 'var': 0.5, 'decay': 0.9}, \n",
    "    'cyclical': {'mode': 'sin', 'decay': 0.9, 'mean': 1.0, 'amp': 1.0, 'period': 20, 'cycle_momentum': False}\n",
    "}\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models\n",
    "\n",
    "|           Models            |      Abbr.       |\n",
    "| --------------------------- | ---------------- |\n",
    "| Deep Neural Networks        | DNN              |\n",
    "| Shrodinger Loss DNN         | S-DNN            |\n",
    "| Energy Loss S-DNN           | SE-DNN           |\n",
    "| Normalized SE-DNN           | NSE-DNN          |\n",
    "| Extended Training S-DNN     | S-DNN$_{ex}$     |\n",
    "| Extended Training SE-DNN    | SE-DNN$_{ex}$    |\n",
    "| Normalized S-DNN$_{ex}$     | NS-DNN$_{ex}$    |\n",
    "| Normalized SE-DNN$_{ex}$    | NSE-DNN$_{ex}$   |\n",
    "| Label-Free S-DNN$_{ex}$     | S-DNN$_{ex}$-LB  |\n",
    "| Label-Free SE-DNN$_{ex}$    | SE-DNN$_{ex}$-LB |\n",
    "| Normalized S-DNN$_{ex}$-LB  | NS-DNN$_{ex}$-LB |\n",
    "| Normalized SE-DNN$_{ex}$-LB | NSE-DNN$_{ex}$-LB|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from training import Trainer\n",
    "from presets import LambdaSearch\n",
    "from config_plots import global_settings\n",
    "from utils import *\n",
    "\n",
    "global_settings()\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_logs = len(\n",
    "    [\n",
    "        name for name in os.listdir(\n",
    "            '../logs/'\n",
    "        ) if os.path.isfile(name)\n",
    "    ]\n",
    ")\n",
    "\n",
    "if num_logs > 2000:\n",
    "    free_logs('../logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Parameters and Start Tasks\n",
    "\n",
    "This part includes customized training parameters. Change these to whatever you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_presets = LambdaSearch(data_path='../datasets/')\n",
    "\n",
    "train_sizes = [100, 200, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
    "break_loop = False\n",
    "loss_plot = False\n",
    "mb = master_bar(range(10))\n",
    "for i in mb:\n",
    "    for train_size in progress_bar(train_sizes):\n",
    "        \n",
    "        # the black-box neural networks\n",
    "        param = param_presets.DNN()\n",
    "        param.name = 'DNN'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.loss_params['lambda_s'] = 0.0\n",
    "        param.loss_params['lambda_e0'] = 0.0\n",
    "        param.loss_params['anneal_factor'] = 0.0\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # PINN-analogue\n",
    "        param = param_presets.NSE_DNNex_LB()\n",
    "        param.name = 'NSE-DNNex-LF'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.loss_params['lambda_s'] = 0.566698\n",
    "        param.loss_params['lambda_e0'] = 3.680050\n",
    "        param.loss_params['anneal_factor'] = 0.786220\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # PGNN-analogue\n",
    "        param = param_presets.NSE_DNNex()\n",
    "        param.name = 'NSE-DNNex'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.loss_params['lambda_s'] = 0.433467\n",
    "        param.loss_params['lambda_e0'] = 2.297982\n",
    "        param.loss_params['anneal_factor'] = 0.861043\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # MTL-PGNN\n",
    "        param = param_presets.vNSE_DNNex()\n",
    "        param.name = 'vNSE-NNex'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.train_params['vanilla'] = True\n",
    "        param.loss_params['lambda_s'] = 0.433467\n",
    "        param.loss_params['lambda_e0'] = 2.297982\n",
    "        param.loss_params['anneal_factor'] = 0.861043\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "        \n",
    "        # CoPhy-PGNN\n",
    "        param = param_presets.NSE_DNNex()\n",
    "        param.name = 'cNSE-NNex'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.loss_params['lambda_s'] = 0.0\n",
    "        param.loss_params['lambda_e0'] = 2.061856\n",
    "        param.loss_params['anneal_factor'] = 0.816932\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.846349,\n",
    "            'threshold': 51.0,\n",
    "            'smooth': 0.171778\n",
    "        }    \n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # CoPhy-PGNN (w/o E-Loss)\n",
    "        param = param_presets.NS_DNNex()\n",
    "        param.name = 'cNS-NNex'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.846349,\n",
    "            'threshold': 51.0,\n",
    "            'smooth': 0.171778\n",
    "        }    \n",
    "        param.loss_params['lambda_s'] = 0.0\n",
    "        param.loss_params['lambda_e0'] = 0.0\n",
    "        param.loss_params['anneal_factor'] = 0.0\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # CoPhy-PGNN (only-D_Tr)\n",
    "        param = param_presets.NSE_DNN()\n",
    "        param.name = 'cNSE-NN'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['lambda_s'] = 0.274606\n",
    "        param.loss_params['lambda_e0'] = 3.046513\n",
    "        param.loss_params['anneal_factor'] = 0.672920\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.846349,\n",
    "            'threshold': 51.0,\n",
    "            'smooth': 0.171778\n",
    "        }\n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)\n",
    "\n",
    "        # CoPhy-PGNN (Label-free)\n",
    "        param = param_presets.NSE_DNNex_LB()\n",
    "        param.name = 'cNSE-NNex-LF'\n",
    "        param.data_params['train_size'] = train_size\n",
    "        param.train_params['break_loop_early'] = break_loop\n",
    "        param.loss_params['lambda_s'] = 0.0\n",
    "        param.loss_params['lambda_e0'] = 3.680050\n",
    "        param.loss_params['anneal_factor'] = 0.786220\n",
    "        param.data_params['device'] = device\n",
    "        param.nn_params['device'] = device\n",
    "        param.loss_params['cold_start'] = {\n",
    "            'mode': 'sigmoid',\n",
    "            'lambda_s': 0.846349,\n",
    "            'threshold': 51.0,\n",
    "            'smooth': 0.171778\n",
    "        }    \n",
    "        trainer = Trainer(master_bar=mb, plot=loss_plot)\n",
    "        trainer.start(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
