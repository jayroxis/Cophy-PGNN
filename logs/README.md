## Logs

Log files are stored here and is mainly used for debugging. It can still give many informations when things do not go smoothly.

An example of a log file *log_-0x1204d67b24af5e8e.txt* looks like this:

~~~~ json
==========================================================================
Action: initialize task.
Time: 02-08-2020-07:43:42
Task Id: -0x1204d67b24af5e8e

io_params:
{
    "path_out": "../results/",
    "path_fig": "../figures/",
    "path_log": "../logs/",
    "env_path": [],
    "use_timestamp": true
}
data_params:
{
    "data_path": "//home/elhamod/melhamodenv/CMT/downloadedData/Ising/",
    "phase": "single-phase",
    "n_sites": "4",
    "train_size": "20000",
    "val_size": "2000",
    "test_size": "0",
    "normalize_input": "True",
    "normalize_output": "False",
    "device": "cuda:0",
    "dataset": "new"
}
nn_params:
{
    "hidden_size": "100",
    "depth": "3",
    "activation": "<class 'torch.nn.modules.activation.Tanh'>",
    "device": "cuda:0"
}
loss_params:
{
    "lambda_s": 1.4181874509958525,
    "lambda_e0": 3.4764449785147944,
    "anneal_factor": 0.6308594975220596,
    "anneal_interval": 10,
    "norm_wf": true,
    "noise": {},
    "cyclical": {}
}
train_params:
{
    "epochs": "500",
    "train_loss": "['phy_loss', 'energy_loss']",
    "test_loss": "['phy_loss', 'energy_loss']",
    "num_batch": "1",
    "optimizer": "<class 'torch.optim.adamax.Adamax'>",
    "cyclical": "{}",
    "L2_reg": "0.0",
    "verbose": "False",
    "print_interval": "10",
    "early_stopping": "{'patience': 50, 'verbose': False, 'delta': 0}"
}

    Status: Successful
--------------------------------------------------------------------------

        
==========================================================================
Action: load data.
Time: 02-08-2020-07:43:42
Task Id: -0x1204d67b24af5e8e

Training Inputs:    (20000, 44)
Training Outputs:   torch.Size([20000, 17])
Validation Inputs:  (2000, 44)
Validation Outputs: torch.Size([2000, 17])
Test Inputs:  (10000, 44)
Test Outputs: torch.Size([10000, 17])

X Scaler: StandardScaler(copy=True, with_mean=True, with_std=True)
Y Scaler: None
Device:   cuda:0
        Status: Successful
--------------------------------------------------------------------------
            
        
==========================================================================
Action: build model.
Time: 02-08-2020-07:43:42
Task Id: -0x1204d67b24af5e8e

Input Dimension:    44
Output Dimension:   17
Model Depth:        3
Hidden State Width: 100

Activation:  <class 'torch.nn.modules.activation.Tanh'>
Device:      cuda:0

        Status: Successful
--------------------------------------------------------------------------
            
        
==========================================================================
Action: training model.
Time: 02-08-2020-07:43:42
Task Id: -0x1204d67b24af5e8e

Number of Epochs:   500
Train Batch Size:   20000
Test Batch Size:    10000
Optimizer:          Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0.0
)

Training Loss:      ['phy_loss', 'energy_loss']
Test Loss:          ['phy_loss', 'energy_loss']

--------------------------------------------------------------------------    
~~~~
This will give enough information to how things go at each step.

Besides that, we also have ***losses files*** as seperate logs of the training process. It is not as detailed as the results file.
An example of loss files *loss_0x7bdc5165d0329958.txt* looks like this

~~~~ json
==========================================================================
Time: 02-08-2020-07:06:26
Task Id: 0x7bdc5165d0329958
Epoch 	 Training 	 Test 		 Loss-Phy 	 Loss-E 	 Anealing Factor
0 	 0.10536279 	 0.13754497 	 0.02259552 	 0.80383915 	 1.61590675
10 	 0.05887193 	 0.04231599 	 0.55054313 	 0.23291144 	 1.33120638
20 	 0.03508028 	 0.04542975 	 0.45054194 	 0.20763274 	 1.09666628
30 	 0.02056759 	 0.03754513 	 0.32217181 	 0.22070122 	 0.90344888
40 	 0.01141428 	 0.02229097 	 0.18286408 	 0.23564140 	 0.74427372
50 	 0.00872987 	 0.01656058 	 0.13334028 	 0.24604495 	 0.61314301
60 	 0.00657188 	 0.01312766 	 0.09483102 	 0.25898421 	 0.50511571
70 	 0.00505488 	 0.01097641 	 0.07030348 	 0.27022400 	 0.41612133
80 	 0.00382612 	 0.00948255 	 0.05179787 	 0.28101337 	 0.34280652
90 	 0.00287175 	 0.00843677 	 0.03810630 	 0.29091853 	 0.28240876
100 	 0.00213920 	 0.00767701 	 0.02789649 	 0.29992947 	 0.23265225
110 	 0.00158332 	 0.00711871 	 0.02028927 	 0.30803403 	 0.19166215
120 	 0.00116762 	 0.00669319 	 0.01466297 	 0.31522998 	 0.15789394
130 	 0.00086089 	 0.00635062 	 0.01053267 	 0.32155567 	 0.13007522
140 	 0.00063750 	 0.00606141 	 0.00752801 	 0.32706252 	 0.10715777
150 	 0.00047665 	 0.00580752 	 0.00536299 	 0.33181366 	 0.08827805
160 	 0.00036186 	 0.00557718 	 0.00381831 	 0.33587790 	 0.07272468
170 	 0.00028038 	 0.00536331 	 0.00272649 	 0.33932838 	 0.05991160
180 	 0.00022259 	 0.00516375 	 0.00196103 	 0.34223896 	 0.04935601
190 	 0.00018155 	 0.00498070 	 0.00142729 	 0.34468079 	 0.04066017
200 	 0.00015237 	 0.00481566 	 0.00105537 	 0.34672016 	 0.03349641
210 	 0.00013153 	 0.00466572 	 0.00079591 	 0.34841755 	 0.02759481
220 	 0.00011640 	 0.00452829 	 0.00061529 	 0.34982666 	 0.02273298
230 	 0.00010518 	 0.00440280 	 0.00048941 	 0.35099402 	 0.01872775
240 	 0.00009670 	 0.00428746 	 0.00040107 	 0.35195962 	 0.01542818
250 	 0.00009013 	 0.00417836 	 0.00033850 	 0.35275769 	 0.01270995
260 	 0.00008488 	 0.00407167 	 0.00029347 	 0.35341704 	 0.01047063
270 	 0.00008058 	 0.00397333 	 0.00026046 	 0.35396099 	 0.00862585
280 	 0.00007696 	 0.00389124 	 0.00023575 	 0.35440901 	 0.00710610
290 	 0.00007387 	 0.00381588 	 0.00021673 	 0.35477853 	 0.00585410
300 	 0.00007118 	 0.00374794 	 0.00020169 	 0.35508290 	 0.00482269
310 	 0.00006880 	 0.00368648 	 0.00018944 	 0.35533366 	 0.00397300
320 	 0.00006669 	 0.00363034 	 0.00017916 	 0.35554025 	 0.00327301
330 	 0.00006477 	 0.00357641 	 0.00017023 	 0.35571039 	 0.00269635
340 	 0.00006304 	 0.00353106 	 0.00016247 	 0.35585019 	 0.00222129
350 	 0.00006161 	 0.00352210 	 0.00015729 	 0.35596648 	 0.00182993
360 	 0.00006017 	 0.00345985 	 0.00014945 	 0.35620901 	 0.00150752
370 	 0.00005906 	 0.00348030 	 0.00014388 	 0.35611084 	 0.00124192
380 	 0.00005764 	 0.00343460 	 0.00013813 	 0.35619941 	 0.00102311
390 	 0.00005641 	 0.00334990 	 0.00013367 	 0.35628229 	 0.00084285
400 	 0.00005578 	 0.00340339 	 0.00013088 	 0.35649872 	 0.00069435
410 	 0.00005475 	 0.00344905 	 0.00012493 	 0.35630375 	 0.00057202
420 	 0.00005346 	 0.00343122 	 0.00012115 	 0.35635230 	 0.00047124
430 	 0.00005228 	 0.00338080 	 0.00011610 	 0.35638461 	 0.00038821
440 	 0.00005130 	 0.00335627 	 0.00011233 	 0.35640284 	 0.00031981
450 	 0.00005035 	 0.00332779 	 0.00011119 	 0.35663381 	 0.00026347
460 	 0.00004989 	 0.00347147 	 0.00010719 	 0.35647970 	 0.00021705
470 	 0.00004951 	 0.00350890 	 0.00010721 	 0.35646099 	 0.00017881
480 	 0.00004831 	 0.00340583 	 0.00009925 	 0.35645461 	 0.00014730
490 	 0.00004745 	 0.00336849 	 0.00009595 	 0.35645735 	 0.00012135
Training stopped at 499/500.Training time: 130.223871 seconds.
Training Complete
--------------------------------------------------------------------------
~~~~
